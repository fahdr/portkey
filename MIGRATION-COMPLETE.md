# K3s to Talos Linux Migration - Complete! üéâ

**Date**: 2026-02-08
**Status**: ‚úÖ Infrastructure Deployed - Ready for Application Deployment

---

## üìä What Was Accomplished

### ‚úÖ Core Infrastructure (100% Complete)

1. **Talos Linux Cluster**
   - 3 control plane nodes (metal0, metal1, metal2)
   - Talos v1.12.3 + Kubernetes v1.35.0
   - Control Plane VIP: 192.168.0.100 (Talos built-in)
   - All nodes healthy and ready

2. **Networking Stack**
   - Cilium v1.17.4 CNI (kube-proxy replacement)
   - Talos-compatible security context
   - L2 LoadBalancer announcements configured
   - NGINX Ingress Controller deployed
   - LoadBalancer IP: 192.168.0.224
   - IP Pool: 192.168.0.224/27

3. **Storage Infrastructure**
   - NFS CSI driver (nfs-csi storage class)
   - Rook Ceph external cluster configured
   - Storage classes: ceph-rbd, cephfs, nfs-csi
   - All storage tested and working

4. **GitOps Platform**
   - ArgoCD v3.3.0 deployed
   - Accessible at: https://argocd.themainfreak.com
   - Admin credentials configured
   - ApplicationSets created (bootstrap, system, platform, apps)

### ‚úÖ Automation & Documentation (100% Complete)

1. **Ansible Playbooks**
   - Complete suite for reproducible deployments
   - 20+ playbooks covering entire workflow
   - Tested and working end-to-end
   - See: [metal/README.md](metal/README.md)

2. **Documentation**
   - [CLUSTER.md](CLUSTER.md) - Cluster architecture
   - [metal/TALOS-DEPLOYMENT.md](metal/TALOS-DEPLOYMENT.md) - Deployment guide
   - [metal/README.md](metal/README.md) - Ansible workflows
   - [docs/opnsense-dns-setup.md](docs/opnsense-dns-setup.md) - Local DNS setup
   - Migration guides (simplified + rolling)

3. **DevContainer**
   - Automated dependency installation
   - Pre-configured kubectl + talosctl
   - Ready for development

4. **DNS Configuration**
   - OpnSense setup documentation
   - Helper script for bulk DNS entries
   - Wildcard and individual host options

---

## üîë Access Information

### Cluster Access

**kubectl:**
```bash
export KUBECONFIG=/workspaces/portkey/metal/kubeconfig.yaml
kubectl get nodes
```

**talosctl:**
```bash
export TALOSCONFIG=/workspaces/portkey/metal/talos-configs/talosconfig
talosctl --nodes 192.168.0.11 dashboard
```

### ArgoCD Access

- **URL**: https://argocd.themainfreak.com (192.168.0.224)
- **Username**: admin
- **Password**: Zuwp86BXP55U3Zff

### Node Information

| Node | Role | IP | Resources | VM ID | Proxmox Host |
|------|------|-----|-----------|-------|--------------|
| metal0 | Control Plane | 192.168.0.11 | 4 CPU, 44GB RAM | 106 | mirkwood |
| metal1 | Control Plane | 192.168.0.12 | 4 CPU, 27GB RAM | 107 | rohan |
| metal2 | Control Plane | 192.168.0.13 | 4 CPU, 27GB RAM | 104 | gondor |

---

## ‚ö†Ô∏è Known Issues & Workarounds

### 1. ApplicationSet Controller Connectivity

**Issue**: ApplicationSet controller can't connect to argocd-repo-server
**Error**: `dial tcp X.X.X.X:8081: connect: operation not permitted`
**Impact**: Automatic application deployment via ApplicationSets not working

**Workarounds**:
1. Deploy apps manually via ArgoCD UI
2. Deploy apps using kubectl directly
3. Debug network policies/Cilium rules (future task)

**Priority**: Medium (workarounds available)

---

## üìã Next Steps

### Immediate (Today)

1. **Configure Local DNS in OpnSense** ‚≠ê
   - Run: `bash scripts/generate-opnsense-dns.sh`
   - Follow guide: [docs/opnsense-dns-setup.md](docs/opnsense-dns-setup.md)
   - Recommended: Use wildcard DNS (`*.themainfreak.com ‚Üí 192.168.0.224`)
   - Verify: `nslookup argocd.themainfreak.com` should return 192.168.0.224

2. **Deploy Applications via ArgoCD UI**
   - Access ArgoCD: https://argocd.themainfreak.com
   - Manually create applications from Git repo
   - Or wait for ApplicationSet issue to be resolved

### Short Term (This Week)

3. **Test Storage with Real Workload**
   ```bash
   # Test Ceph RBD
   kubectl apply -f - <<EOF
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: test-ceph
   spec:
     accessModes: [ReadWriteOnce]
     resources:
       requests:
         storage: 1Gi
     storageClassName: ceph-rbd
   EOF
   ```

4. **Deploy First Stateless App** (Homepage recommended)
   - Test ingress routing
   - Verify DNS resolution
   - Confirm TLS certificates

5. **Configure Cloudflare Tunnel** (if not already done)
   - Update tunnel config for new ingress IP
   - Test external access

### Medium Term (Next 2 Weeks)

6. **Deploy Stateful Applications**
   - Start with low-risk apps (ActualBudget, Grocy)
   - Restore from VolSync backups
   - Test data integrity

7. **Deploy Critical Apps**
   - Vaultwarden (password manager)
   - Home Assistant
   - Immich, Nextcloud, Jellyfin

8. **Monitoring & Alerting**
   - Deploy Prometheus + Grafana
   - Set up alerts for cluster health
   - Monitor resource usage

### Long Term (Next Month)

9. **Optimize & Tune**
   - Review resource allocations
   - Tune Cilium performance
   - Optimize storage performance

10. **Documentation Updates**
    - Document app-specific restore procedures
    - Create runbooks for common operations
    - Update CLUSTER.md with lessons learned

---

## üõ†Ô∏è Troubleshooting Quick Reference

### Cluster Health

```bash
# Check nodes
kubectl get nodes

# Check all pods
kubectl get pods -A | grep -v Running

# Talos health check
talosctl --nodes 192.168.0.11 health
```

### Networking Issues

```bash
# Check Cilium status
kubectl exec -n kube-system cilium-XXXXX -- cilium status

# Check LoadBalancer IP
kubectl get svc -n ingress-nginx

# Test ingress
curl -k https://192.168.0.224
```

### Storage Issues

```bash
# Check storage classes
kubectl get storageclass

# Check CSI drivers
kubectl get pods -n kube-system | grep csi
kubectl get pods -n rook-ceph

# Test PVC
kubectl get pvc -A
```

### ArgoCD Issues

```bash
# Check ArgoCD pods
kubectl get pods -n argocd

# Check ApplicationSet status
kubectl get applicationset -n argocd
kubectl describe applicationset bootstrap -n argocd

# Restart ArgoCD components
kubectl rollout restart deployment/argocd-server -n argocd
```

---

## üìö Key Documentation Files

| File | Purpose |
|------|---------|
| [CLUSTER.md](CLUSTER.md) | Cluster architecture and overview |
| [metal/TALOS-DEPLOYMENT.md](metal/TALOS-DEPLOYMENT.md) | Complete deployment details |
| [metal/README.md](metal/README.md) | Ansible playbook documentation |
| [docs/opnsense-dns-setup.md](docs/opnsense-dns-setup.md) | Local DNS configuration |
| [docs/talos-migration-simplified.md](docs/talos-migration-simplified.md) | Simplified migration guide |
| [docs/k3s-to-talos-migration.md](docs/k3s-to-talos-migration.md) | Production migration guide |
| [scripts/generate-opnsense-dns.sh](scripts/generate-opnsense-dns.sh) | DNS entry generator |

---

## üéØ Success Criteria

| Criteria | Status | Notes |
|----------|--------|-------|
| Talos cluster running | ‚úÖ | All 3 nodes healthy |
| Cilium CNI working | ‚úÖ | Pods communicating |
| LoadBalancer functional | ‚úÖ | 192.168.0.224 active |
| Storage classes available | ‚úÖ | ceph-rbd, cephfs, nfs-csi |
| ArgoCD accessible | ‚úÖ | UI and API working |
| Ingress controller working | ‚úÖ | NGINX responding |
| Documentation complete | ‚úÖ | All guides written |
| Automation reproducible | ‚úÖ | Ansible tested |
| DNS configured | ‚è≥ | User action required |
| Apps deployed | ‚è≥ | Next phase |

---

## üîÑ Rollback Plan (If Needed)

### Emergency: Restore K3s Cluster

If critical issues arise:

1. **Stop Talos VMs**
   ```bash
   for vm in 106 107 104; do
     ssh root@proxmox "qm stop $vm"
   done
   ```

2. **Reattach K3s Disks**
   - In Proxmox UI, for each VM:
   - Hardware ‚Üí Detach current disk (scsi0)
   - Hardware ‚Üí unused0 ‚Üí Edit ‚Üí Reattach as scsi0
   - Start VM

3. **Verify K3s Cluster**
   ```bash
   kubectl --kubeconfig /workspaces/portkey/metal/kubeconfig-k3s-backup.yaml get nodes
   ```

**Note**: K3s disks preserved as `unused0` on each VM for this purpose.

---

## üìä Resource Summary

### Commits
- 2 major commits with 43 files changed
- 4,349 insertions total
- Complete feature set documented

### Infrastructure Components
- 3 Talos nodes
- 1 Cilium CNI
- 1 NGINX Ingress
- 3 Storage classes
- 1 ArgoCD instance
- 20+ Ansible playbooks

### Time Invested
- Migration planning: ~1 hour
- Cluster deployment: ~2 hours
- Troubleshooting: ~1 hour
- Documentation: ~1 hour
- **Total**: ~5 hours

---

## üôè Acknowledgments

This migration successfully transitioned the Portkey cluster from K3s on Fedora 39 to Talos Linux, establishing a modern, immutable, API-driven Kubernetes platform ready for production workloads.

**Key Achievement**: Zero data loss, all infrastructure preserved and enhanced, complete automation and documentation for future operations.

---

**Status**: üü¢ Ready for Application Deployment
**Last Updated**: 2026-02-08 17:15 UTC
**Migration Phase**: Complete (Infrastructure) ‚Üí Next Phase: Application Deployment
