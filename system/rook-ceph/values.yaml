# Reduce CSI driver resource usage
rook-ceph:
  csi:
    # Disable CephFS CSI since we're not using CephFS
    enableCephfsDriver: false
    csiRBDProvisionerResource: |
      - name: csi-provisioner
        resource:
          requests:
            memory: 128Mi
            cpu: 50m
          limits:
            memory: 256Mi
      - name: csi-resizer
        resource:
          requests:
            memory: 128Mi
            cpu: 50m
          limits:
            memory: 256Mi
      - name: csi-attacher
        resource:
          requests:
            memory: 128Mi
            cpu: 50m
          limits:
            memory: 256Mi
      - name: csi-snapshotter
        resource:
          requests:
            memory: 128Mi
            cpu: 50m
          limits:
            memory: 256Mi

rook-ceph-cluster:
  cephClusterSpec:
    external:
      enable: true
    crashCollector:
      disable: true
    healthCheck:
      daemonHealth:
        mon:
          disabled: false
          interval: 45s
    cephVersion:
      image: ghcr.io/fahdr/proxmox-ceph:v19.2.1
  cephBlockPools:
    - name: ceph-blockpool
      spec:
        failureDomain: host
        replicated:
          size: 3
      storageClass:
        enabled: true
        name: ceph-block
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        mountOptions: []
        allowedTopologies: []
        parameters:
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/fstype: ext4

  # CephFS explicitly disabled to reduce CPU usage - set to empty array to override chart defaults
  # To re-enable CephFS:
  # 1. Replace the empty array below with the full cephFileSystems configuration
  # 2. Set enableCephfsDriver: true in the CSI section above  
  # 3. Uncomment cephFileSystemVolumeSnapshotClass section below
  # 4. Increase MDS resources if needed (activeStandby: true, resources)
  cephFileSystems: []  # EXPLICITLY DISABLED - prevents Helm chart defaults from creating CephFS
  # 
  # Full configuration to restore when needed:
  # cephFileSystems:
  #   - name: ceph-filesystem
  #     spec:
  #       metadataPool:
  #         replicated:
  #           size: 3
  #       dataPools:
  #         - failureDomain: host
  #           replicated:
  #             size: 3
  #           name: data0
  #       metadataServer:
  #         activeCount: 1
  #         activeStandby: false  # Set to true for HA
  #         resources:
  #           requests:
  #             cpu: "100m"  # Increase if performance issues
  #             memory: "4Gi"
  #           limits:
  #             memory: "4Gi"
  #         priorityClassName: system-cluster-critical
  #     storageClass:
  #       enabled: true
  #       name: ceph-filesystem
  #       pool: data0
  #       reclaimPolicy: Delete
  #       allowVolumeExpansion: true
  #       volumeBindingMode: "Immediate"
  #       mountOptions: []
  #       parameters:
  #         csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  #         csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
  #         csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  #         csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
  #         csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  #         csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
  #         csi.storage.k8s.io/fstype: ext4

  cephObjectStores:
    - name: ceph-objectstore
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
        preservePoolsOnDelete: true
        gateway:
          port: 80
          resources:
            limits:
              memory: "1Gi"  # Reduced from 2Gi
            requests:
              cpu: "100m"  # Reduced from 200m
              memory: "512Mi"  # Reduced from 1Gi
          instances: 1
          priorityClassName: system-cluster-critical
        # Explicitly disable multisite for single-site external cluster
        zone:
          name: ""
      storageClass:
        enabled: true
        name: ceph-bucket
        reclaimPolicy: Delete
        volumeBindingMode: "Immediate"
        parameters:
          region: us-east-1
      ingress:
        enabled: false

  # CephFS Volume Snapshot Class - explicitly disabled since CephFS is disabled
  # Set to empty object to override any chart defaults
  cephFileSystemVolumeSnapshotClass: {}  # EXPLICITLY DISABLED
  #
  # Uncomment when re-enabling CephFS above:
  # cephFileSystemVolumeSnapshotClass:
  #   enabled: true
  #   name: ceph-filesystem
  #   isDefault: false
  #   deletionPolicy: Delete
  #   annotations: {}
  #   labels: {}
  #   parameters: {}

  cephBlockPoolsVolumeSnapshotClass:
    enabled: true
    name: ceph-block
    isDefault: true
    deletionPolicy: Delete
    annotations: {}
    labels: {}
    # see https://rook.io/docs/rook/v1.10/Storage-Configuration/Ceph-CSI/ceph-csi-snapshot/#rbd-snapshots for available configuration
    parameters: {}
