# Talos Linux Cluster Automation

Ansible playbooks for deploying and managing a Talos Linux Kubernetes cluster on Proxmox.

## Cluster Overview

- **OS**: Talos Linux v1.12.3 (Image Factory with i915-ucode extension)
- **Kubernetes**: v1.35.0
- **Nodes**: 3 control plane + worker nodes
- **Networking**: Cilium 1.17.4 CNI with kube-proxy replacement
- **Storage**: Rook Ceph (external cluster), NFS CSI
- **GPU**: Intel i915 via Talos system extension + intel-device-plugins-operator

### Nodes

| Node | IP | VM ID | Proxmox Host | Bridge | Role | CPU | RAM |
|------|-----|-------|--------------|--------|------|-----|-----|
| metal0 | 192.168.0.11 | 106 | mirkwood | vmbr1 | Control Plane | 4 | 44GB |
| metal1 | 192.168.0.12 | 107 | rohan | vmbr1 | Control Plane | 4 | 27GB |
| metal2 | 192.168.0.13 | 104 | gondor | vmbr1 | Control Plane | 4 | 27GB |
| metal3 | 192.168.0.14 | 114 | rivendell | vmbr0 | Worker | 4 | 10GB |

> **Note on bridges**: mirkwood/rohan/gondor use `vmbr1` for LAN (vmbr0 = WAN via OPNsense). rivendell uses `vmbr0` for LAN directly. Set `vm_network_bridge` per-node in the inventory.

### Network Configuration

- **Control Plane VIP**: 192.168.0.100 (Talos built-in VIP)
- **Pod CIDR**: 10.0.1.0/8
- **Service CIDR**: 10.43.0.0/16
- **LoadBalancer Pool**: 192.168.0.224/27 (Cilium L2 announcements)

## Prerequisites

### Required Tools

```bash
# Ansible 2.15+
ansible-galaxy collection install community.general kubernetes.core

# talosctl CLI
curl -sL https://talos.dev/install | sh

# Python dependencies (needed for DHCP IP discovery and YAML processing)
pip install kubernetes pyyaml
```

### Proxmox API Credentials

Create a `.env` file in this directory:

```bash
PROXMOX_HOST=192.168.0.2
PROXMOX_USER=root@pam
PROXMOX_TOKEN_ID=ansible-claude
PROXMOX_TOKEN_SECRET=your-token-secret
```

### Talos ISO

The Talos ISO (`talos-v1.12.3-amd64.iso`) must be uploaded to `local` storage on each Proxmox node where VMs will be created.

## Directory Structure

```
metal/
  .env                          # Proxmox API credentials (gitignored)
  Makefile                      # Top-level targets
  inventories/
    talos.yml                   # Node inventory (IPs, VM specs, Proxmox hosts)
  playbooks/
    talos-create-vms.yml        # Create and start VMs on Proxmox
    talos-add-node.yml          # Add a node to an existing cluster
    talos-apply-configs.yml     # Apply Talos configs to all nodes
    talos-post-bootstrap.yml    # Post-bootstrap: kubeconfig + Cilium + CRDs
    talos-upgrade.yml           # Rolling upgrade
    ...
  roles/
    talos_config/
      defaults/main.yml         # Cluster settings (CIDRs, extensions, cert SANs)
      tasks/main.yml            # Config generation + DHCP discovery + apply
      templates/
        common-patch.yaml.j2    # All nodes: DNS, NTP, kernel, CNI, subnets
        controlplane-patch.yaml.j2  # CP nodes: endpoint, scheduling
        worker-patch.yaml.j2    # Worker nodes: (placeholder for customization)
        node-patch.yaml.j2      # Per-node: hostname, IP, interface, VIP
  talos-configs/
    controlplane.yaml           # Base control plane config (generated once)
    worker.yaml                 # Base worker config (generated once)
    talosconfig                 # Talos client config
    patches/                    # Templated patches (generated by Ansible)
      common.yaml
      controlplane.yaml
      worker.yaml
      metal0.yaml ... metalN.yaml
```

## How the Config System Works

Talos machine configs are built from a **base config + layered patches**:

```
Base config (controlplane.yaml or worker.yaml)
  + common patch       (DNS, NTP, kernel modules, CIDRs, Cilium, API server args)
  + role patch          (CP: endpoint + scheduling, Worker: placeholder)
  + node patch          (hostname, static IP, network interface, VIP)
  = Final merged config (applied to the node)
```

The base configs are generated once with `talosctl gen config` and contain cluster secrets. Patches are templated from Ansible inventory variables on each run.

**Multi-document YAML handling**: `talosctl gen config` produces multi-document YAML (machine config + HostnameConfig). Since `--config-patch` doesn't work with multi-document files, the role extracts the first document, merges patches with `talosctl machineconfig patch`, then applies the merged result.

**DHCP IP discovery**: New VMs boot from the Talos ISO with a DHCP address, not their target static IP. The role automatically:
1. Checks if the node is reachable at its static IP
2. If not, gets the VM's MAC address from the Proxmox API
3. Scans the subnet for Talos API (port 50000), excluding known nodes
4. Applies the config to the discovered DHCP IP
5. Waits for the node to reboot with its static IP

## Adding a Worker Node

### Step 1: Add to Inventory

Edit `inventories/talos.yml` and add the node under `talos_workers.hosts`:

```yaml
talos_workers:
  hosts:
    metal3:
      ansible_host: 192.168.0.14       # Target static IP
      proxmox_node: rivendell           # Proxmox host to create VM on
      proxmox_host: 192.168.0.202      # Proxmox host IP (for API)
      vm_id: 114                        # Proxmox VM ID
      cpu: 4
      memory: 10240                     # RAM in MB
      disk_size: 50                     # Disk in GB
      network_interface: ens18          # NIC name inside Talos VM
      vm_network_bridge: vmbr0          # Proxmox bridge (vmbr0 or vmbr1)
```

### Step 2: Create the VM

```bash
cd metal
ansible-playbook -i inventories/talos.yml playbooks/talos-create-vms.yml -l metal3
```

This creates the VM on Proxmox, attaches the Talos ISO, and starts it. The VM will boot into Talos maintenance mode with a DHCP address.

### Step 3: Apply Config and Join Cluster

Wait ~60 seconds for the VM to boot, then:

```bash
ansible-playbook -i inventories/talos.yml playbooks/talos-add-node.yml -l metal3
```

This will:
1. Template all patch files from inventory
2. Discover the VM's DHCP IP (automatic subnet scan)
3. Merge patches into the base worker config
4. Apply the merged config to the DHCP IP
5. Wait for the node to reboot with its static IP
6. Verify the node joined the Kubernetes cluster

### Step 4: Verify

```bash
kubectl get nodes -o wide
# metal3 should appear as Ready within ~60 seconds
```

> **Kubelet serving certificates**: New nodes need their kubelet serving cert CSRs approved for metrics-server and Prometheus to scrape them. The `kubelet-csr-approver` controller (deployed via `system/kubelet-csr-approver/`) handles this automatically. If it's not yet deployed, approve manually: `kubectl certificate approve $(kubectl get csr -o name)`

## Complete Deployment Workflow (Fresh Cluster)

### Step 1: Create VMs

```bash
ansible-playbook -i inventories/talos.yml playbooks/talos-create-vms.yml
```

### Step 2: Apply Talos Configurations

```bash
# Wait ~60s for VMs to boot, then:
ansible-playbook -i inventories/talos.yml playbooks/talos-apply-configs.yml
```

### Step 3: Bootstrap the Cluster

```bash
talosctl --talosconfig talos-configs/talosconfig bootstrap --nodes 192.168.0.11
```

### Step 4: Post-Bootstrap Setup

```bash
# Deploy Cilium CNI, configure networking, get kubeconfig
ansible-playbook playbooks/talos-post-bootstrap.yml
```

### Step 5: Deploy Storage

```bash
ansible-playbook playbooks/talos-deploy-storage.yml
```

### Step 6: Deploy Ingress Controller

```bash
ansible-playbook playbooks/talos-deploy-ingress.yml
```

### Step 7: Bootstrap GitOps (Secrets + ArgoCD)

```bash
cd /workspaces/portkey
make bootstrap
# (prompts for Ansible vault password)
```

### Step 8: External Secrets (Terraform)

```bash
make external
```

### Full Bootstrap (One-liner)

```bash
cd /workspaces/portkey
make          # metal + bootstrap + external
make post-install  # Kanidm OAuth setup (needs running pods)
```

## Common Operations

### Access the Cluster

```bash
export TALOSCONFIG=$(pwd)/talos-configs/talosconfig
export KUBECONFIG=$(pwd)/kubeconfig.yaml

talosctl health
kubectl get nodes -o wide
talosctl dashboard
```

### Upgrade Talos

```bash
# Rolling upgrade (one node at a time)
ansible-playbook -i inventories/talos.yml playbooks/talos-upgrade.yml
```

Or manually per-node (use the Image Factory image to preserve extensions):

```bash
talosctl --nodes 192.168.0.11 upgrade \
  --image factory.talos.dev/installer/<schematic-id>:<talos-version>
```

> **Important**: Always use the Image Factory URL (not `ghcr.io/siderolabs/installer`) so system extensions like i915-ucode are included. The schematic ID is in `roles/talos_config/defaults/main.yml`.

### Upgrade Kubernetes

```bash
talosctl upgrade-k8s \
  --nodes 192.168.0.11,192.168.0.12,192.168.0.13 \
  --to 1.36.0
```

### View Logs

```bash
talosctl logs kubelet --nodes 192.168.0.11
talosctl logs etcd --nodes 192.168.0.11
talosctl dmesg --nodes 192.168.0.11
```

### Reboot a Node

```bash
talosctl reboot --nodes 192.168.0.12
```

## Playbook Reference

| Playbook | Purpose | Usage |
|----------|---------|-------|
| `talos-create-vms.yml` | Create VMs on Proxmox from Talos ISO | `-l <node>` to target specific nodes |
| `talos-add-node.yml` | Add node to existing cluster (discover IP, apply config, verify) | `-l <node>` required |
| `talos-apply-configs.yml` | Apply machine configs (for fresh deploy, all nodes) | Optional `-l <node>` |
| `talos-post-bootstrap.yml` | Kubeconfig + Cilium + networking + CRDs | Run once after bootstrap |
| `talos-install-cilium.yml` | Install Cilium CNI (called by post-bootstrap) | |
| `talos-deploy-cilium.yml` | Alternative Cilium install via Ansible helm module | |
| `talos-configure-networking.yml` | L2 announcements and LB IP pools | |
| `talos-install-crds.yml` | Pre-install CRDs to prevent race conditions | |
| `talos-deploy-storage.yml` | Deploy Rook Ceph and NFS CSI | |
| `talos-deploy-ingress.yml` | Deploy NGINX Ingress Controller | |
| `talos-upgrade.yml` | Rolling upgrade of all nodes (one at a time) | |
| `talos-upload-iso.yml` | Upload Talos ISO to Proxmox nodes | |

## Inventory Variables Reference

### Global Variables (`all.vars`)

| Variable | Description | Default |
|----------|-------------|---------|
| `control_plane_endpoint` | VIP for API server | `192.168.0.100` |
| `talos_version` | Talos Linux version | `v1.12.3` |
| `kubernetes_version` | Kubernetes version | `v1.33.0` |

### Per-Node Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `ansible_host` | Target static IP address | Yes |
| `proxmox_node` | Proxmox host name | Yes |
| `proxmox_host` | Proxmox host IP | Yes |
| `vm_id` | Proxmox VM ID | Yes |
| `cpu` | Number of CPU cores | Yes |
| `memory` | RAM in MB | Yes |
| `disk_size` | Disk size in GB | No (default: 50) |
| `network_interface` | NIC name in Talos (ens18, eth0) | Yes |
| `vm_network_bridge` | Proxmox bridge name | No (default: vmbr1) |
| `talos_vip_enabled` | Assign VIP to this node | No (default: false) |

### Role Defaults (`talos_config/defaults/main.yml`)

| Variable | Description |
|----------|-------------|
| `pod_subnet` | Pod CIDR (10.0.1.0/8) |
| `service_subnet` | Service CIDR (10.43.0.0/16) |
| `dns_servers` | DNS resolvers |
| `cert_sans` | API server certificate SANs |
| `talos_image_factory_schematic` | Image Factory schematic ID for extensions |
| `kernel_modules` | Kernel modules to load (i915) |

## Troubleshooting

### Node not accessible

```bash
# Check if Talos API is responding
talosctl version --nodes 192.168.0.11

# Check VM console via Proxmox UI to see boot status
```

### New node stuck on DHCP / didn't get config

```bash
# Find what IP the node got via DHCP (scan for Talos API port)
python3 -c "
import socket, concurrent.futures
def check(ip):
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.settimeout(0.5)
        r = s.connect_ex((ip, 50000))
        s.close()
        return ip if r == 0 else None
    except: return None
with concurrent.futures.ThreadPoolExecutor(max_workers=50) as ex:
    found = [r for r in ex.map(check, [f'192.168.0.{i}' for i in range(1,255)]) if r]
print('Talos API found at:', found)
"

# Manually apply config to a known DHCP IP
talosctl apply-config --insecure --nodes 192.168.0.57 --file /tmp/metal3-config.yaml
```

### VM bridge error on Proxmox

If VM creation fails with `bridge 'vmbr1' does not exist`:
- Check which bridges exist: Proxmox UI > Node > Network
- Set `vm_network_bridge` in inventory for that node
- rivendell uses `vmbr0`, mirkwood/rohan/gondor use `vmbr1`

### Kubelet metrics not working on new node (TLS error)

If `kubectl top node <name>` shows `<unknown>` or metrics-server logs show `tls: internal error`, the kubelet serving certificate CSR hasn't been approved:

```bash
# Check for pending CSRs
kubectl get csr

# If kubelet-csr-approver is deployed, CSRs are approved automatically.
# Otherwise, approve manually:
kubectl certificate approve <csr-name>
```

The `kubelet-csr-approver` controller (`system/kubelet-csr-approver/`) auto-approves these CSRs for nodes matching IP prefix `192.168.0.0/24` and hostname patterns `metalN` or `talos-*`.

### Cilium not starting on new worker

```bash
kubectl get pods -n kube-system -l app.kubernetes.io/name=cilium
kubectl logs -n kube-system -l app.kubernetes.io/name=cilium --tail=50
```

### Config patch fails with "JSON6902 patches not supported"

This happens when the base config is multi-document YAML. The `talos_config` role handles this automatically by extracting the first document before patching. If applying manually:

```bash
# Extract first document
python3 -c "import yaml; docs=list(yaml.safe_load_all(open('worker.yaml'))); yaml.dump(docs[0], open('/tmp/worker-single.yaml','w'), default_flow_style=False)"

# Patch and apply
talosctl machineconfig patch /tmp/worker-single.yaml \
  --patch @patches/common.yaml \
  --patch @patches/worker.yaml \
  --patch @patches/metal3.yaml \
  --output /tmp/metal3-config.yaml

talosctl apply-config --insecure --nodes <DHCP-IP> --file /tmp/metal3-config.yaml
```

## System Extensions (Image Factory)

Talos uses an immutable root filesystem. Kernel modules like i915 (Intel GPU) are added via [Talos Image Factory](https://factory.talos.dev).

### Current Extensions

| Extension | Purpose |
|-----------|---------|
| `i915-ucode` | Intel GPU firmware + i915 kernel module |

### Adding a New Extension

1. Find the extension name from [Talos Extensions](https://github.com/siderolabs/extensions)
2. Generate a new schematic:
   ```bash
   curl -X POST https://factory.talos.dev/schematics \
     -H "Content-Type: application/json" \
     -d '{"customization":{"systemExtensions":{"officialExtensions":[
       "siderolabs/i915-ucode",
       "siderolabs/NEW-EXTENSION"
     ]}}}'
   ```
3. Update `talos_image_factory_schematic` in `roles/talos_config/defaults/main.yml`
4. Run the upgrade playbook: `ansible-playbook -i inventories/talos.yml playbooks/talos-upgrade.yml`

### Verifying Extensions

```bash
talosctl -n 192.168.0.11 get extensions
talosctl -n 192.168.0.11 read /proc/modules | grep i915
talosctl -n 192.168.0.11 ls /dev/dri/
```

---

**Last Updated**: 2026-02-10
**Status**: 4-node cluster (3 CP + 1 worker) running Talos v1.12.3 / K8s v1.35.0
