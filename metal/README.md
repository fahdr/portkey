# Talos Linux Cluster Automation

Ansible playbooks for deploying and managing a Talos Linux Kubernetes cluster on Proxmox.

## Cluster Overview

- **OS**: Talos Linux v1.12.3 (Image Factory with i915-ucode extension)
- **Kubernetes**: v1.35.0
- **Nodes**: 3 control plane + worker nodes
- **Networking**: Cilium 1.17.4 CNI with kube-proxy replacement
- **Storage**: Rook Ceph (external cluster), NFS CSI
- **GPU**: Intel i915 via Talos system extension + intel-device-plugins-operator

### Proxmox Hosts

| Host | IP | Role | Notes |
|------|-----|------|-------|
| shire | 192.168.0.2 | Cluster seed, OPNsense, NFS | 8-disk ZFS, NFS/Samba via LXC |
| rivendell | 192.168.0.202 | K8s worker host | LAN on vmbr0 |
| isengard | 192.168.0.102 | Ceph MON | |
| mirkwood | 192.168.0.8 | K8s CP host (metal0) | AMD Ryzen, LAN on vmbr1 |
| rohan | 192.168.0.7 | K8s CP host (metal1) | Intel, LAN on vmbr1 |
| gondor | 192.168.0.6 | K8s CP host (metal2) | Intel, LAN on vmbr1 |
| erebor | 192.168.0.101 | K8s host | NVIDIA GTX 1660 Ti, LAN on vmbr0 |

### Kubernetes Nodes (Talos VMs)

| Node | IP | VM ID | Proxmox Host | Bridge | Role | CPU | RAM |
|------|-----|-------|--------------|--------|------|-----|-----|
| metal0 | 192.168.0.11 | 106 | mirkwood | vmbr1 | Control Plane | 4 | 44GB |
| metal1 | 192.168.0.12 | 107 | rohan | vmbr1 | Control Plane | 4 | 27GB |
| metal2 | 192.168.0.13 | 104 | gondor | vmbr1 | Control Plane | 4 | 27GB |
| metal3 | 192.168.0.14 | 114 | rivendell | vmbr0 | Worker | 4 | 10GB |

> **Note on bridges**: mirkwood/rohan/gondor use `vmbr1` for LAN (vmbr0 = WAN via OPNsense). rivendell/erebor use `vmbr0` for LAN directly. Set `vm_network_bridge` per-node in the inventory.

### Network Configuration

- **Control Plane VIP**: 192.168.0.100 (Talos built-in VIP)
- **Pod CIDR**: 10.0.1.0/8
- **Service CIDR**: 10.43.0.0/16
- **LoadBalancer Pool**: 192.168.0.224/27 (Cilium L2 announcements)

## Prerequisites

### Required Tools

```bash
# Ansible 2.15+ with required collections
ansible-galaxy collection install -r requirements.yml

# talosctl CLI
curl -sL https://talos.dev/install | sh

# Python dependencies (needed for DHCP IP discovery and YAML processing)
pip install kubernetes pyyaml
```

### Proxmox API Credentials

Create a `.env` file in this directory:

```bash
PROXMOX_HOST=192.168.0.2
PROXMOX_USER=root@pam
PROXMOX_TOKEN_ID=ansible-claude
PROXMOX_TOKEN_SECRET=your-token-secret
```

### Talos ISO

The Talos ISO (`talos-v1.12.3-amd64.iso`) must be uploaded to `local` storage on each Proxmox node where VMs will be created.

## Directory Structure

```
metal/
  .env                          # Proxmox API credentials (gitignored)
  vault.yml                     # Ansible vault (Proxmox root password, gitignored)
  Makefile                      # Top-level targets
  requirements.yml              # Ansible collection dependencies
  ansible.cfg                   # Ansible configuration
  inventories/
    proxmox.yml                 # Proxmox host inventory (physical nodes)
    talos.yml                   # Talos node inventory (K8s VMs)
  playbooks/
    proxmox-setup-node.yml      # Setup Proxmox nodes (repos, packages, GPU, cluster join)
    talos-create-vms.yml        # Create VMs on Proxmox from Talos ISO
    talos-add-node.yml          # Add a node to an existing cluster
    talos-apply-configs.yml     # Apply Talos configs to all nodes
    talos-post-bootstrap.yml    # Post-bootstrap: kubeconfig + Cilium + CRDs
    talos-upgrade.yml           # Rolling upgrade
    ...
  roles/
    proxmox_setup/
      defaults/main.yml         # Package list, cluster join settings
      tasks/
        main.yml                # Repos, packages, Ceph, sysctl, fail2ban, cluster join
        gpu-passthrough.yml     # NVIDIA discrete GPU passthrough (VFIO-PCI binding)
        igpu-passthrough.yml    # Intel iGPU passthrough (i915 blacklist, IOMMU)
        resize-root.yml         # Shrink root LV via initramfs (free space for thin pool)
        safe-reboot.yml         # K8s-aware reboot (drain, reboot, wait, uncordon)
      handlers/main.yml         # Service restart handlers
    talos_config/
      defaults/main.yml         # Cluster settings (CIDRs, extensions, cert SANs)
      tasks/main.yml            # Config generation + DHCP discovery + apply
      templates/
        common-patch.yaml.j2    # All nodes: DNS, NTP, kernel, CNI, subnets
        controlplane-patch.yaml.j2  # CP nodes: endpoint, scheduling
        worker-patch.yaml.j2    # Worker nodes: (placeholder for customization)
        node-patch.yaml.j2      # Per-node: hostname, IP, interface, VIP
  talos-configs/
    controlplane.yaml           # Base control plane config (generated once)
    worker.yaml                 # Base worker config (generated once)
    talosconfig                 # Talos client config
    patches/                    # Templated patches (generated by Ansible)
      common.yaml
      controlplane.yaml
      worker.yaml
      metal0.yaml ... metalN.yaml
```

## How the Config System Works

Talos machine configs are built from a **base config + layered patches**:

```
Base config (controlplane.yaml or worker.yaml)
  + common patch       (DNS, NTP, kernel modules, CIDRs, Cilium, API server args)
  + role patch          (CP: endpoint + scheduling, Worker: placeholder)
  + node patch          (hostname, static IP, network interface, VIP)
  = Final merged config (applied to the node)
```

The base configs are generated once with `talosctl gen config` and contain cluster secrets. Patches are templated from Ansible inventory variables on each run.

**Multi-document YAML handling**: `talosctl gen config` produces multi-document YAML (machine config + HostnameConfig). Since `--config-patch` doesn't work with multi-document files, the role extracts the first document, merges patches with `talosctl machineconfig patch`, then applies the merged result.

**DHCP IP discovery**: New VMs boot from the Talos ISO with a DHCP address, not their target static IP. The role automatically:
1. Checks if the node is reachable at its static IP
2. If not, gets the VM's MAC address from the Proxmox API
3. Scans the subnet for Talos API (port 50000), excluding known nodes
4. Applies the config to the discovered DHCP IP
5. Waits for the node to reboot with its static IP

## Setting Up a New Proxmox Node

### Step 1: Add to Inventory

Edit `inventories/proxmox.yml` and add the node under `proxmox_new.hosts`:

```yaml
proxmox_new:
  hosts:
    erebor:
      ansible_host: 192.168.0.101
      proxmox_join_cluster: true
      proxmox_create_vmbr1: false        # vmbr0 is LAN on this node
      proxmox_root_size: "50G"            # Optional: shrink root LV
      gpu_passthrough_ids: "10de:2182,10de:1aeb,10de:1aec,10de:1aed"  # Optional: NVIDIA GPU
      gpu_pci_address: "01:00"            # PCI address of the GPU
```

### Step 2: Create the Vault (First Time Only)

The vault stores the Proxmox root password for automated cluster join:

```bash
cd metal
ansible-vault create vault.yml
# Add: proxmox_root_password: "your-root-password"
```

### Step 3: Run the Playbook

```bash
cd metal
ansible-playbook -i inventories/proxmox.yml playbooks/proxmox-setup-node.yml \
  -l erebor --ask-vault-pass
```

This will:
1. Disable enterprise repos, enable no-subscription repos
2. Remove the Proxmox subscription nag
3. Run a full system upgrade
4. Install 40+ essential packages (monitoring, networking, storage tools)
5. Install Ceph Squid packages via `pveceph`
6. Apply sysctl performance and security tuning
7. Deploy SSH authorized key
8. Create vmbr1 bridge (if `proxmox_create_vmbr1: true`)
9. Configure fail2ban for Proxmox web UI
10. Join the Proxmox cluster (via `pvecm add` with password from vault)
11. Resize root LV (if `proxmox_root_size` is set) — see [Root LV Resize](#root-lv-resize)
12. Configure NVIDIA GPU passthrough (if `gpu_passthrough_ids` is set) — see [GPU Passthrough](#gpu-passthrough)
13. Configure Intel iGPU passthrough (if `igpu_passthrough: true`) — see [GPU Passthrough](#gpu-passthrough)

### Step 4: Verify

```bash
# Check in Proxmox UI
https://192.168.0.101:8006

# Check cluster membership
ssh root@<node-ip> pvecm status
```

### Moving to Existing

Once a node is set up, move it from `proxmox_new` to `proxmox_existing` in the inventory to avoid re-running cluster join on subsequent playbook runs.

### Running Against Existing Nodes

The playbook targets all `proxmox` hosts and processes them one at a time (`serial: 1`). This is safe for existing nodes — all tasks are idempotent. Use `-l` to target specific nodes:

```bash
# Run against a single existing node (e.g., apply GPU passthrough)
ansible-playbook -i inventories/proxmox.yml playbooks/proxmox-setup-node.yml \
  -l rohan --ask-vault-pass

# Run against all nodes (serial: 1 ensures one at a time)
ansible-playbook -i inventories/proxmox.yml playbooks/proxmox-setup-node.yml \
  --ask-vault-pass
```

## GPU Passthrough

The playbook supports two GPU passthrough modes, configured via inventory variables.

### NVIDIA Discrete GPU (VFIO-PCI)

For NVIDIA cards (e.g., GTX 1660 Ti on erebor), VFIO-PCI binding is configured at boot:

```yaml
# inventories/proxmox.yml
erebor:
  gpu_type: "NVIDIA GTX 1660 Ti"
  gpu_passthrough_ids: "10de:2182,10de:1aeb,10de:1aec,10de:1aed"  # All PCI functions
  gpu_pci_address: "01:00"
```

What it does:
1. Enables IOMMU (`intel_iommu=on iommu=pt`) in GRUB kernel parameters
2. Configures `vfio-pci.ids` to pre-bind the GPU at boot
3. Adds VFIO modules to initramfs
4. Blacklists nouveau/nvidia/nvidiafb drivers on the host
5. Configures KVM MSR ignore (prevents NVIDIA VM crashes)
6. Reboots safely (draining K8s VMs first if applicable)
7. Verifies IOMMU, VFIO modules, and GPU driver status

After setup, pass the GPU to a VM:
```bash
qm set <VMID> -hostpci0 01:00,pcie=1,x-vga=1
```

> **Finding PCI IDs**: Run `lspci -nn | grep -i nvidia` on the host to find all PCI function IDs for the GPU.

### Intel iGPU (i915 Blacklist)

For Intel integrated GPUs (e.g., Alder Lake-N UHD on rohan/gondor), the i915 driver is blacklisted so the iGPU can be passed through to VMs:

```yaml
# inventories/proxmox.yml
rohan:
  igpu_passthrough: true
  igpu_pci_address: "00:02"
```

What it does:
1. Checks runtime state first (IOMMU enabled? i915 blacklisted? VFIO loaded?)
2. **Skips all changes and reboots if already configured** (idempotent)
3. Enables IOMMU with ACS override for IOMMU group isolation
4. Blacklists i915, nouveau, nvidia, snd_hda_intel via kernel command line
5. Loads VFIO modules at boot via `/etc/modules`
6. Reboots safely if changes were made

After setup, pass the iGPU to a VM:
```bash
qm set <VMID> -hostpci0 00:02,pcie=1
```

> **Note on AMD GPUs**: AMD GPUs (e.g., Renoir on mirkwood) have a known reset bug that prevents reliable passthrough. This is not automated.

### Safe Reboot with K8s Awareness

All reboot-triggering tasks (GPU passthrough, root resize) use `safe-reboot.yml`, which handles Kubernetes gracefully:

1. **Drain**: `kubectl drain` each K8s node on the host (cordon + evict pods)
2. **Reboot**: Reboot the Proxmox host (300s timeout)
3. **Wait**: Pause 30s for VMs to start, then `kubectl wait --for=condition=Ready` with retries
4. **Uncordon**: `kubectl uncordon` to allow scheduling again
5. **Verify**: Print cluster node status

This requires `k8s_vm_nodes` in the inventory to map Proxmox hosts to their K8s VMs:

```yaml
# inventories/proxmox.yml
rohan:
  k8s_vm_nodes: [metal1]     # K8s nodes running on this Proxmox host
gondor:
  k8s_vm_nodes: [metal2]
```

If `k8s_vm_nodes` is not defined for a host, it reboots normally without drain/uncordon.

Combined with `serial: 1` in the playbook, this ensures only one Proxmox host reboots at a time, maintaining cluster quorum.

## Root LV Resize

Proxmox defaults to a large root LV (often ~96G), leaving less space for the `pve/data` thin pool used for VM storage. The resize task shrinks the root LV and extends the thin pool.

### Configuration

```yaml
# inventories/proxmox.yml
erebor:
  proxmox_root_size: "50G"    # Target size for root LV
```

### How It Works

The resize is performed safely via an initramfs premount script that runs **before** root is mounted:

1. **Pre-check** (Ansible): Reads current LV size and disk usage. If usage >= target - 5G, prints a warning and **skips** the resize (playbook continues normally)
2. **Install initramfs scripts**: Adds `resize2fs` and `e2fsck` to initramfs, plus a premount script
3. **Reboot**: Triggers a safe reboot (with K8s drain if applicable)
4. **Initramfs resize** (runs at boot, before root mount):
   - `e2fsck -f -y` (filesystem check, required before shrink)
   - `resize2fs` to 2G below target (safety margin)
   - `lvreduce` to target size
   - `resize2fs` to fill LV (recover safety margin)
5. **Post-reboot**: Verifies sizes, extends `pve/data` thin pool with freed space, cleans up initramfs scripts

### Safety Features

- **Disk usage check**: If root uses more than target - 5G, the resize is skipped with a warning (playbook continues)
- **Initramfs abort**: If `resize2fs` shrink fails (filesystem too full), the script aborts before `lvreduce` — no data loss
- **Idempotent**: If root is already at or below target size, everything is skipped
- **Cleanup**: Initramfs resize scripts are removed after successful resize

### Example Output

```
Root LV:    50.00G
Filesystem: /dev/mapper/pve-root   49G  7.2G   40G  16% /
Thin pool:  399.87G
Extend:     Logical volume pve/data successfully resized
```

## Adding a Worker Node

### Step 1: Add to Inventory

Edit `inventories/talos.yml` and add the node under `talos_workers.hosts`:

```yaml
talos_workers:
  hosts:
    metal3:
      ansible_host: 192.168.0.14       # Target static IP
      proxmox_node: rivendell           # Proxmox host to create VM on
      proxmox_host: 192.168.0.202      # Proxmox host IP (for API)
      vm_id: 114                        # Proxmox VM ID
      cpu: 4
      memory: 10240                     # RAM in MB
      disk_size: 50                     # Disk in GB
      network_interface: ens18          # NIC name inside Talos VM
      vm_network_bridge: vmbr0          # Proxmox bridge (vmbr0 or vmbr1)
```

### Step 2: Create the VM

```bash
cd metal
ansible-playbook -i inventories/talos.yml playbooks/talos-create-vms.yml -l metal3
```

This creates the VM on Proxmox, attaches the Talos ISO, and starts it. The VM will boot into Talos maintenance mode with a DHCP address.

### Step 3: Apply Config and Join Cluster

Wait ~60 seconds for the VM to boot, then:

```bash
ansible-playbook -i inventories/talos.yml playbooks/talos-add-node.yml -l metal3
```

This will:
1. Template all patch files from inventory
2. Discover the VM's DHCP IP (automatic subnet scan)
3. Merge patches into the base worker config
4. Apply the merged config to the DHCP IP
5. Wait for the node to reboot with its static IP
6. Verify the node joined the Kubernetes cluster

### Step 4: Verify

```bash
kubectl get nodes -o wide
# metal3 should appear as Ready within ~60 seconds
```

> **Kubelet serving certificates**: New nodes need their kubelet serving cert CSRs approved for metrics-server and Prometheus to scrape them. The `kubelet-csr-approver` controller (deployed via `system/kubelet-csr-approver/`) handles this automatically. If it's not yet deployed, approve manually: `kubectl certificate approve $(kubectl get csr -o name)`

## Complete Deployment Workflow (Fresh Cluster)

### Step 1: Create VMs

```bash
ansible-playbook -i inventories/talos.yml playbooks/talos-create-vms.yml
```

### Step 2: Apply Talos Configurations

```bash
# Wait ~60s for VMs to boot, then:
ansible-playbook -i inventories/talos.yml playbooks/talos-apply-configs.yml
```

### Step 3: Bootstrap the Cluster

```bash
talosctl --talosconfig talos-configs/talosconfig bootstrap --nodes 192.168.0.11
```

### Step 4: Post-Bootstrap Setup

```bash
# Deploy Cilium CNI, configure networking, get kubeconfig
ansible-playbook playbooks/talos-post-bootstrap.yml
```

### Step 5: Deploy Storage

```bash
ansible-playbook playbooks/talos-deploy-storage.yml
```

### Step 6: Deploy Ingress Controller

```bash
ansible-playbook playbooks/talos-deploy-ingress.yml
```

### Step 7: Bootstrap GitOps (Secrets + ArgoCD)

```bash
cd /workspaces/portkey
make bootstrap
# (prompts for Ansible vault password)
```

### Step 8: External Secrets (Terraform)

```bash
make external
```

### Full Bootstrap (One-liner)

```bash
cd /workspaces/portkey
make          # metal + bootstrap + external
make post-install  # Kanidm OAuth setup (needs running pods)
```

## Common Operations

### Access the Cluster

```bash
export TALOSCONFIG=$(pwd)/talos-configs/talosconfig
export KUBECONFIG=$(pwd)/kubeconfig.yaml

talosctl health
kubectl get nodes -o wide
talosctl dashboard
```

### Upgrade Talos

```bash
# Rolling upgrade (one node at a time)
ansible-playbook -i inventories/talos.yml playbooks/talos-upgrade.yml
```

Or manually per-node (use the Image Factory image to preserve extensions):

```bash
talosctl --nodes 192.168.0.11 upgrade \
  --image factory.talos.dev/installer/<schematic-id>:<talos-version>
```

> **Important**: Always use the Image Factory URL (not `ghcr.io/siderolabs/installer`) so system extensions like i915-ucode are included. The schematic ID is in `roles/talos_config/defaults/main.yml`.

### Upgrade Kubernetes

```bash
talosctl upgrade-k8s \
  --nodes 192.168.0.11,192.168.0.12,192.168.0.13 \
  --to 1.36.0
```

### View Logs

```bash
talosctl logs kubelet --nodes 192.168.0.11
talosctl logs etcd --nodes 192.168.0.11
talosctl dmesg --nodes 192.168.0.11
```

### Reboot a Node

```bash
talosctl reboot --nodes 192.168.0.12
```

## Playbook Reference

| Playbook | Inventory | Purpose | Usage |
|----------|-----------|---------|-------|
| `proxmox-setup-node.yml` | `proxmox.yml` | Setup Proxmox node (repos, packages, GPU, resize, cluster join). Runs `serial: 1`. | `-l <node> --ask-vault-pass` |
| `talos-create-vms.yml` | `talos.yml` | Create VMs on Proxmox from Talos ISO | `-l <node>` to target specific nodes |
| `talos-add-node.yml` | `talos.yml` | Add node to existing cluster (discover IP, apply config, verify) | `-l <node>` required |
| `talos-apply-configs.yml` | `talos.yml` | Apply machine configs (for fresh deploy, all nodes) | Optional `-l <node>` |
| `talos-post-bootstrap.yml` | `talos.yml` | Kubeconfig + Cilium + networking + CRDs | Run once after bootstrap |
| `talos-install-cilium.yml` | `talos.yml` | Install Cilium CNI (called by post-bootstrap) | |
| `talos-deploy-cilium.yml` | `talos.yml` | Alternative Cilium install via Ansible helm module | |
| `talos-configure-networking.yml` | `talos.yml` | L2 announcements and LB IP pools | |
| `talos-install-crds.yml` | `talos.yml` | Pre-install CRDs to prevent race conditions | |
| `talos-deploy-storage.yml` | `talos.yml` | Deploy Rook Ceph and NFS CSI | |
| `talos-deploy-ingress.yml` | `talos.yml` | Deploy NGINX Ingress Controller | |
| `talos-upgrade.yml` | `talos.yml` | Rolling upgrade of all nodes (one at a time) | |
| `talos-upload-iso.yml` | `talos.yml` | Upload Talos ISO to Proxmox nodes | |

## Inventory Variables Reference

### Proxmox Inventory (`inventories/proxmox.yml`)

#### Global Variables (`all.vars`)

| Variable | Description | Default |
|----------|-------------|---------|
| `ansible_user` | SSH user for Proxmox hosts | `root` |
| `proxmox_cluster_seed` | Existing cluster node IP for `pvecm add` | `192.168.0.2` |
| `ceph_repo` | Ceph repo matching cluster version | `ceph-squid` |

#### Per-Node Variables (Proxmox Hosts)

| Variable | Description | Required | Default |
|----------|-------------|----------|---------|
| `ansible_host` | Proxmox host IP | Yes | — |
| `proxmox_join_cluster` | Join the Proxmox cluster on setup | No | `false` |
| `proxmox_create_vmbr1` | Create vmbr1 bridge for K8s VMs | No | `true` |
| `proxmox_root_size` | Target root LV size (e.g., `"50G"`) | No | — (skip resize) |
| `k8s_vm_nodes` | List of K8s node names running on this host | No | `[]` |
| `gpu_passthrough_ids` | NVIDIA PCI device IDs for VFIO-PCI binding | No | — (skip GPU) |
| `gpu_type` | GPU description (for logging) | No | `"NVIDIA"` |
| `gpu_pci_address` | PCI address of discrete GPU | No | `"01:00"` |
| `igpu_passthrough` | Enable Intel iGPU passthrough | No | `false` |
| `igpu_pci_address` | PCI address of Intel iGPU | No | `"00:02"` |

### Talos Inventory (`inventories/talos.yml`)

#### Global Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `control_plane_endpoint` | VIP for API server | `192.168.0.100` |
| `talos_version` | Talos Linux version | `v1.12.3` |
| `kubernetes_version` | Kubernetes version | `v1.33.0` |

#### Per-Node Variables (Talos VMs)

| Variable | Description | Required |
|----------|-------------|----------|
| `ansible_host` | Target static IP address | Yes |
| `proxmox_node` | Proxmox host name | Yes |
| `proxmox_host` | Proxmox host IP | Yes |
| `vm_id` | Proxmox VM ID | Yes |
| `cpu` | Number of CPU cores | Yes |
| `memory` | RAM in MB | Yes |
| `disk_size` | Disk size in GB | No (default: 50) |
| `network_interface` | NIC name in Talos (ens18, eth0) | Yes |
| `vm_network_bridge` | Proxmox bridge name | No (default: vmbr1) |
| `talos_vip_enabled` | Assign VIP to this node | No (default: false) |

### Role Defaults (`talos_config/defaults/main.yml`)

| Variable | Description |
|----------|-------------|
| `pod_subnet` | Pod CIDR (10.0.1.0/8) |
| `service_subnet` | Service CIDR (10.43.0.0/16) |
| `dns_servers` | DNS resolvers |
| `cert_sans` | API server certificate SANs |
| `talos_image_factory_schematic` | Image Factory schematic ID for extensions |
| `kernel_modules` | Kernel modules to load (i915) |

## Troubleshooting

### Node not accessible

```bash
# Check if Talos API is responding
talosctl version --nodes 192.168.0.11

# Check VM console via Proxmox UI to see boot status
```

### New node stuck on DHCP / didn't get config

```bash
# Find what IP the node got via DHCP (scan for Talos API port)
python3 -c "
import socket, concurrent.futures
def check(ip):
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.settimeout(0.5)
        r = s.connect_ex((ip, 50000))
        s.close()
        return ip if r == 0 else None
    except: return None
with concurrent.futures.ThreadPoolExecutor(max_workers=50) as ex:
    found = [r for r in ex.map(check, [f'192.168.0.{i}' for i in range(1,255)]) if r]
print('Talos API found at:', found)
"

# Manually apply config to a known DHCP IP
talosctl apply-config --insecure --nodes 192.168.0.57 --file /tmp/metal3-config.yaml
```

### VM bridge error on Proxmox

If VM creation fails with `bridge 'vmbr1' does not exist`:
- Check which bridges exist: Proxmox UI > Node > Network
- Set `vm_network_bridge` in inventory for that node
- rivendell uses `vmbr0`, mirkwood/rohan/gondor use `vmbr1`

### Kubelet metrics not working on new node (TLS error)

If `kubectl top node <name>` shows `<unknown>` or metrics-server logs show `tls: internal error`, the kubelet serving certificate CSR hasn't been approved:

```bash
# Check for pending CSRs
kubectl get csr

# If kubelet-csr-approver is deployed, CSRs are approved automatically.
# Otherwise, approve manually:
kubectl certificate approve <csr-name>
```

The `kubelet-csr-approver` controller (`system/kubelet-csr-approver/`) auto-approves these CSRs for nodes matching IP prefix `192.168.0.0/24` and hostname patterns `metalN` or `talos-*`.

### Cilium not starting on new worker

```bash
kubectl get pods -n kube-system -l app.kubernetes.io/name=cilium
kubectl logs -n kube-system -l app.kubernetes.io/name=cilium --tail=50
```

### Config patch fails with "JSON6902 patches not supported"

This happens when the base config is multi-document YAML. The `talos_config` role handles this automatically by extracting the first document before patching. If applying manually:

```bash
# Extract first document
python3 -c "import yaml; docs=list(yaml.safe_load_all(open('worker.yaml'))); yaml.dump(docs[0], open('/tmp/worker-single.yaml','w'), default_flow_style=False)"

# Patch and apply
talosctl machineconfig patch /tmp/worker-single.yaml \
  --patch @patches/common.yaml \
  --patch @patches/worker.yaml \
  --patch @patches/metal3.yaml \
  --output /tmp/metal3-config.yaml

talosctl apply-config --insecure --nodes <DHCP-IP> --file /tmp/metal3-config.yaml
```

## System Extensions (Image Factory)

Talos uses an immutable root filesystem. Kernel modules like i915 (Intel GPU) are added via [Talos Image Factory](https://factory.talos.dev).

### Current Extensions

| Extension | Purpose |
|-----------|---------|
| `i915-ucode` | Intel GPU firmware + i915 kernel module |

### Adding a New Extension

1. Find the extension name from [Talos Extensions](https://github.com/siderolabs/extensions)
2. Generate a new schematic:
   ```bash
   curl -X POST https://factory.talos.dev/schematics \
     -H "Content-Type: application/json" \
     -d '{"customization":{"systemExtensions":{"officialExtensions":[
       "siderolabs/i915-ucode",
       "siderolabs/NEW-EXTENSION"
     ]}}}'
   ```
3. Update `talos_image_factory_schematic` in `roles/talos_config/defaults/main.yml`
4. Run the upgrade playbook: `ansible-playbook -i inventories/talos.yml playbooks/talos-upgrade.yml`

### Verifying Extensions

```bash
talosctl -n 192.168.0.11 get extensions
talosctl -n 192.168.0.11 read /proc/modules | grep i915
talosctl -n 192.168.0.11 ls /dev/dri/
```

---

**Last Updated**: 2026-02-10
**Status**: 7-node Proxmox cluster, 4-node K8s cluster (3 CP + 1 worker) running Talos v1.12.3 / K8s v1.35.0
