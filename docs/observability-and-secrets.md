# Portkey Observability & Secret Management

This document describes the observability stack and secret management system used in the Portkey homelab infrastructure.

## Table of Contents

1. [Observability Stack](#observability-stack)
   - [Architecture Overview](#architecture-overview)
   - [Monitored Infrastructure](#monitored-infrastructure)
   - [Exporters and Scraping](#exporters-and-scraping)
   - [Dashboards](#dashboards)
   - [Alerting](#alerting)
2. [Secret Management](#secret-management)
   - [Architecture Overview](#secret-architecture-overview)
   - [Secret Generator](#secret-generator)
   - [Sync Directions](#sync-directions)
   - [ClusterSecretStores](#clustersecretstores)
   - [Consuming Secrets](#consuming-secrets)

---

## Observability Stack

### Architecture Overview

The observability stack is built on `kube-prometheus-stack` (Helm chart version 72.3.1) and provides comprehensive monitoring for the homelab infrastructure.

```
┌─────────────────────────────────────────────────────────────────────┐
│                         Prometheus                                   │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐               │
│  │   Proxmox    │  │   OPNsense   │  │   AdGuard    │               │
│  │   Exporter   │  │   Exporter   │  │   Exporter   │               │
│  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘               │
│         │                 │                 │                        │
│         ▼                 ▼                 ▼                        │
│  ┌─────────────────────────────────────────────────────────────┐    │
│  │                    Prometheus Server                         │    │
│  │              (scrapes metrics via ServiceMonitors)           │    │
│  └─────────────────────────┬───────────────────────────────────┘    │
│                            │                                         │
│         ┌──────────────────┼──────────────────┐                     │
│         ▼                  ▼                  ▼                     │
│  ┌────────────┐     ┌────────────┐     ┌────────────┐              │
│  │  Grafana   │     │Alertmanager│     │    Loki    │              │
│  │ Dashboards │     │  + ntfy.sh │     │   (logs)   │              │
│  └────────────┘     └────────────┘     └────────────┘              │
└─────────────────────────────────────────────────────────────────────┘
```

**Core Components:**
- **Prometheus** - Metrics collection and storage
- **Alertmanager** - Alert routing and notification management
- **Grafana** - Visualization (deployed separately)
- **Loki** - Log aggregation
- **ntfy.sh** - Push notifications for alerts

### Monitored Infrastructure

The stack monitors a 6-node Proxmox cluster and associated services:

| Component | Address | Port | Scrape Interval |
|-----------|---------|------|-----------------|
| Proxmox VE (6 nodes) | 192.168.0.2, .202, .102, .8, .7, .6 | 8006 | 60s |
| OPNsense Firewall | 192.168.0.1 | 443 | 30s |
| AdGuard DNS | 192.168.0.1 | 3000 | 30s |
| Ceph Storage (3 mgrs) | shire, rivendell, isengard | 9283 | 30s |
| Kubernetes | Internal | Various | Default |

**Proxmox Nodes:**
- `shire` (192.168.0.2) - Primary node with ZFS storage
- `rivendell` (192.168.0.202)
- `isengard` (192.168.0.102)
- `mirkwood` (192.168.0.8)
- `rohan` (192.168.0.7)
- `gondor` (192.168.0.6)

### Exporters and Scraping

#### Proxmox Exporter

**Image:** `prompve/prometheus-pve-exporter:3.4.5`

Collects metrics from all Proxmox nodes via the PVE API. Uses token-based authentication with credentials stored in Vaultwarden.

**Configuration:**
```yaml
# pve.yml generated by initContainer
default:
  user: prometheus@pve
  token_name: prometheus
  token_value: <from-secret>
  verify_ssl: false
```

**Key Metrics:**
- `pve_up` - Node availability
- `pve_cpu_usage_ratio` - CPU utilization
- `pve_memory_usage_bytes` - Memory consumption
- `pve_storage_*` - Storage pool metrics
- `pve_guest_info` - VM/container status

#### OPNsense Exporter

**Image:** `ghcr.io/athennamind/opnsense-exporter:latest`

Collects firewall and system metrics from OPNsense via its REST API.

**Key Metrics:**
- `opnsense_up` - Exporter connectivity status
- `opnsense_interfaces_received_bytes_total` / `opnsense_interfaces_transmitted_bytes_total` - Traffic counters
- `opnsense_interfaces_mtu_bytes` - Interface MTU
- `opnsense_firewall_in_ipv4_pass_packets` / `opnsense_firewall_in_ipv4_block_packets` - Firewall packet counters
- `opnsense_firewall_out_ipv4_pass_packets` / `opnsense_firewall_out_ipv4_block_packets` - Outbound firewall counters
- `opnsense_gateways_rtt_milliseconds` - Gateway round-trip time
- `opnsense_gateways_loss_percentage` - Gateway packet loss
- `opnsense_protocol_tcp_connection_count_by_state` - TCP connection states
- `opnsense_services_status` - Service status (1=running, 0=stopped)
- `opnsense_services_running_total` / `opnsense_services_stopped_total` - Service counts
- `opnsense_arp_table_entries` - ARP table with IP, MAC, and interface labels
- `opnsense_firmware_*` - Firmware version and update status

**API Permissions Required:**
The OPNsense API user needs sufficient permissions to access:
- `api/core/system/status`
- `api/diagnostics/traffic/interface`
- `api/diagnostics/firewall/pf_statistics/interfaces`
- `api/routes/gateway/status`

> **Note:** Some endpoints like `api/dhcpv4/leases/searchLease` may require admin-level permissions.

**Known Limitations:**
- DHCP leases are not exported as Prometheus metrics (high cardinality data)
- Unbound DNS stats may fail if Unbound is not the primary DNS resolver
- Some service/cron endpoints require elevated permissions

#### AdGuard Exporter

**Image:** `ebrianne/adguard-exporter:latest`

Collects DNS filtering metrics from AdGuard Home.

**Key Metrics:**
- `adguard_dns_queries_total`
- `adguard_blocked_filtering`
- `adguard_avg_processing_time`
- `adguard_protection_enabled`
- `adguard_query_types` - Query type breakdown

#### SNMP Exporter

**Image:** `prom/snmp-exporter:v0.26.0`

Collects system metrics from SNMP-enabled devices. Currently configured to scrape OPNsense for interface statistics.

**Configuration:**
```yaml
infrastructure:
  snmp:
    enabled: true
    imageTag: "v0.26.0"
    targets:
      - name: "opnsense"
        address: "192.168.0.1"
        module: "if_mib"
```

**Module: if_mib**
Standard IF-MIB for interface statistics:
- `sysUpTime` - System uptime
- `sysName` - System hostname
- `ifNumber` - Number of interfaces
- `ifDescr` - Interface descriptions
- `ifOperStatus` - Interface operational state
- `ifInOctets` / `ifOutOctets` - Traffic counters (32-bit)
- `ifHCInOctets` / `ifHCOutOctets` - Traffic counters (64-bit)

**Prerequisites:**
OPNsense requires the `os-net-snmp` plugin to be installed and configured:
1. System → Firmware → Packages → Install `os-net-snmp`
2. Services → Net-SNMP → Enable SNMP agent
3. Configure community string (default: `public`)

> **Note:** SNMP plugin installation may require a firmware upgrade if OPNsense is running an older version.

#### Ceph External Cluster

Scrapes the native Ceph Prometheus endpoint running on Proxmox MGR nodes.

**Key Metrics:**
- `ceph_health_status` - Cluster health (0=OK, 1=WARN, 2=ERR)
- `ceph_osd_up` / `ceph_osd_in` - OSD availability
- `ceph_cluster_total_bytes` / `ceph_cluster_total_used_bytes`
- `ceph_pg_*` - Placement group metrics
- `ceph_pool_*` - Pool-level statistics

### Dashboards

Six pre-configured Grafana dashboards are available:

| Dashboard | Description |
|-----------|-------------|
| **Infrastructure Overview** | Single pane of glass for all components |
| **Proxmox Cluster** | Node resources, VMs, containers, storage |
| **OPNsense Firewall** | Network traffic, firewall, services, network devices |
| **AdGuard DNS** | Query rates, blocking stats, response times |
| **Ceph Cluster** | Health, OSDs, capacity, performance |
| **Homelab Applications** | App health, pod restarts, resource usage |

Dashboards are stored as JSON ConfigMaps and auto-provisioned to Grafana.

#### Proxmox Cluster Dashboard

**Key Panels:**
- **Cluster Overview** - Total nodes, running VMs/containers, cluster CPU/memory
- **Node Status** - Per-node resource utilization
- **Virtual Machines Table** - VM name, node, status, CPU, memory (uses `pve_guest_info{type="qemu"}`)
- **Containers Table** - LXC name, node, status, CPU, memory (uses `pve_guest_info{type="lxc"}`)
- **Storage Pools** - Capacity and usage per storage

**Metric Patterns:**
- Node metrics use `id` label with format `node/<nodename>`
- VM metrics use `id` label with format `qemu/<vmid>`
- Container metrics use `id` label with format `lxc/<vmid>`
- Guest info (name, node) comes from `pve_guest_info` metric

#### OPNsense Firewall Dashboard

**Sections:**

1. **System Overview**
   - OPNsense Status (up/down)
   - Packets/sec throughput
   - Running/Stopped services count
   - ARP entries count
   - TCP connections count

2. **Network Traffic**
   - Interface inbound traffic (bps)
   - Interface outbound traffic (bps)

3. **Gateways**
   - Gateway RTT (latency monitoring)
   - Gateway packet loss percentage

4. **Firewall**
   - Passed packets (IPv4/IPv6 in/out)
   - Blocked packets (IPv4/IPv6 in/out)

5. **Protocol Statistics**
   - TCP connections by state
   - UDP traffic rates

6. **Services**
   - Table of all OPNsense services with status (Running/Stopped)
   - Includes Kea DHCP, Unbound DNS, etc.

7. **Network Devices (ARP Table)**
   - IP Address
   - MAC Address
   - Interface
   - Type (ethernet)
   - Permanent (Static/Dynamic)

> **Note:** DHCP leases with hostnames are not available as the OPNsense exporter doesn't expose Kea DHCP lease data as Prometheus metrics. The ARP table provides network device presence information.

### Alerting

40+ alert rules cover all infrastructure layers:

**Proxmox Alerts:**
- `ProxmoxExporterDown` - Critical (5m)
- `ProxmoxNodeHighCPU` - Warning (>90%, 15m)
- `ProxmoxNodeHighMemory` - Warning (>90%, 15m)
- `ProxmoxStorageCritical` - Critical (>95%, 15m)

**OPNsense Alerts:**
- `OPNsenseDown` - Critical (2m)
- `OPNsenseHighCPU` - Warning (>80%, 10m)
- `OPNsenseInterfaceDown` - Critical (2m)

**AdGuard Alerts:**
- `AdGuardDown` - Critical (5m)
- `AdGuardProtectionDisabled` - Warning (5m)

**Ceph Alerts:**
- `CephClusterError` - Critical (HEALTH_ERR, 2m)
- `CephOSDDown` - Critical (5m)
- `CephStorageCritical` - Critical (>90%, 15m)

**Application Alerts:**
- `VaultwardenDown` - Critical (2m)
- `HomeAssistantDown` - Warning (5m)
- `ArgocdUnhealthy` - Warning (15m)

Alerts route through Alertmanager to ntfy.sh for push notifications.

---

## Secret Management

### Secret Architecture Overview

The global-secrets platform synchronizes secrets between Kubernetes and Vaultwarden (self-hosted password manager).

```
┌─────────────────────────────────────────────────────────────────────┐
│                     Secret Generator Job                             │
│  ┌─────────────────────────────────────────────────────────────┐    │
│  │  1. Parse config.yaml                                        │    │
│  │  2. Check existing secrets in K8s and Vaultwarden            │    │
│  │  3. Generate missing passwords                               │    │
│  │  4. Sync based on syncDirection policy                       │    │
│  └─────────────────────────┬───────────────────────────────────┘    │
│                            │                                         │
│         ┌──────────────────┴──────────────────┐                     │
│         ▼                                      ▼                     │
│  ┌────────────────────┐              ┌────────────────────┐         │
│  │  Kubernetes Secret │              │  Vaultwarden Item  │         │
│  │  (global-secrets)  │              │   (Secure Note)    │         │
│  └─────────┬──────────┘              └────────────────────┘         │
│            │                                                         │
│            ▼                                                         │
│  ┌─────────────────────────────────────────────────────────────┐    │
│  │              External Secrets Operator (ESO)                 │    │
│  │     ClusterSecretStore: global-secrets (K8s provider)        │    │
│  └─────────────────────────┬───────────────────────────────────┘    │
│                            │                                         │
│         ┌──────────────────┼──────────────────┐                     │
│         ▼                  ▼                  ▼                     │
│  ┌────────────┐     ┌────────────┐     ┌────────────┐              │
│  │  Grafana   │     │    Dex     │     │   Other    │              │
│  │  Secrets   │     │  Secrets   │     │   Apps     │              │
│  └────────────┘     └────────────┘     └────────────┘              │
└─────────────────────────────────────────────────────────────────────┘
```

### Secret Generator

The secret-generator is a Go application that runs as a Kubernetes Job.

**Location:** `platform/global-secrets/files/secret-generator/`

**How It Works:**

1. **Parse Configuration** - Reads `config.yaml` for secret definitions
2. **Check Existing Secrets** - Queries both Kubernetes and Vaultwarden
3. **Determine Source of Truth** - Based on `syncDirection` setting
4. **Generate Missing Values** - Creates random passwords for undefined keys
5. **Sync to Targets** - Updates Kubernetes secrets and/or Vaultwarden items

**Configuration Format:**
```yaml
- name: grafana-admin              # Secret name
  syncTo: ["kubernetes", "vaultwarden"]  # Sync targets
  syncDirection: "vaultwarden-to-kubernetes"  # Source of truth
  data:
    - key: password               # Field name
      length: 40                  # Password length
      special: true               # Include special characters
```

**Managed Secrets:**

| Secret Name | Fields | Sync Direction |
|-------------|--------|----------------|
| `dex.grafana` | client-secret | kubernetes-to-vaultwarden |
| `registry.admin` | password | kubernetes-to-vaultwarden |
| `paperless.admin` | PAPERLESS_ADMIN_PASSWORD | kubernetes-to-vaultwarden |
| `home-assistant` | home-assistant | kubernetes-to-vaultwarden |
| `vaultwarden` | postgres-password, admin-token | kubernetes-to-vaultwarden |
| `case10-mariadb` | root_password, user_password, repl_password | kubernetes-to-vaultwarden |
| `case10-minio` | access_key, secret_key | kubernetes-to-vaultwarden |
| `grafana-admin` | password | vaultwarden-to-kubernetes |

### Sync Directions

Three sync directions control where the source of truth resides:

| Direction | Description | Use Case |
|-----------|-------------|----------|
| `kubernetes-to-vaultwarden` | K8s is primary, syncs to Vaultwarden | Auto-generated secrets |
| `vaultwarden-to-kubernetes` | Vaultwarden is primary, syncs to K8s | Manually managed secrets |
| `bidirectional` | Merges both sources (K8s preferred) | Shared secrets |

**Default:** `kubernetes-to-vaultwarden`

### ClusterSecretStores

Four ClusterSecretStores provide different access patterns:

#### global-secrets (Kubernetes Provider)
```yaml
secretStoreRef:
  kind: ClusterSecretStore
  name: global-secrets
```
- Reads K8s secrets from `global-secrets` namespace
- Used by most applications
- Fastest refresh (direct K8s API)

#### vaultwarden-fields (Webhook Provider)
```yaml
secretStoreRef:
  kind: ClusterSecretStore
  name: vaultwarden-fields
```
- Reads custom fields from Vaultwarden items
- JSONPath: `$.data.fields[?@.name=={property}].value`
- Direct Vaultwarden access via CLI API

#### vaultwarden-login (Webhook Provider)
```yaml
secretStoreRef:
  kind: ClusterSecretStore
  name: vaultwarden-login
```
- Reads login credentials (username/password) from Vaultwarden
- JSONPath: `$.data.login.{property}`
- For traditional login-type items

#### vaultwarden-notes (Webhook Provider)
```yaml
secretStoreRef:
  kind: ClusterSecretStore
  name: vaultwarden-notes
```
- Reads notes field from Vaultwarden items
- JSONPath: `$.data.notes`
- For text-based secrets

### Consuming Secrets

Applications consume secrets using ExternalSecret resources:

```yaml
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: grafana-admin-secret
  namespace: grafana
spec:
  refreshInterval: 1h
  secretStoreRef:
    kind: ClusterSecretStore
    name: global-secrets
  target:
    name: grafana-admin-secret
    template:
      data:
        admin-user: "admin"
        admin-password: "{{ .password }}"
  data:
    - secretKey: password
      remoteRef:
        key: grafana-admin      # Name from config.yaml
        property: password      # Field from config.yaml
```

**Workflow:**
1. ExternalSecret references `global-secrets` ClusterSecretStore
2. ESO reads `grafana-admin` secret from `global-secrets` namespace
3. ESO extracts `password` field
4. ESO creates target secret `grafana-admin-secret` in `grafana` namespace
5. Secret auto-refreshes based on `refreshInterval`

### Vaultwarden CLI Service

The vaultwarden-cli service provides REST API access to Vaultwarden:

**Image:** `ghcr.io/charlesthomas/bitwarden-cli:2025.6.1`
**Service:** `vaultwarden-cli.global-secrets.svc.cluster.local:8087`

**API Endpoints:**
- `GET /object/item/{name}` - Fetch item by name
- `POST /object/item` - Create new item
- `PUT /object/item/{id}` - Update existing item
- `DELETE /object/item/{id}` - Delete item
- `GET /list/object/items` - List all items

The secret-generator uses these endpoints to sync secrets to Vaultwarden.

---

## File Structure

```
platform/global-secrets/
├── Chart.yaml
├── files/
│   └── secret-generator/
│       ├── main.go           # Go implementation
│       └── config.yaml       # Secret definitions
└── templates/
    ├── clustersecretstore/   # K8s-based secret store
    ├── secret-generator/     # Job and RBAC
    └── vaultwarden/          # CLI service and webhook stores

system/monitoring-system/
├── Chart.yaml
├── values.yaml               # All exporter configs
├── files/
│   ├── dashboards/           # Grafana JSON dashboards
│   └── webhook-transformer/  # Alert transformer
└── templates/
    ├── infrastructure-monitoring.yaml   # Exporter deployments
    ├── *-servicemonitor.yaml            # Scrape configs
    ├── infrastructure-rules.yaml        # Alert rules
    └── exporter-secrets.yaml            # Credential ExternalSecrets
```

---

## Troubleshooting

### Secret Generator Issues

**Check job status:**
```bash
kubectl get jobs -n global-secrets --sort-by=.metadata.creationTimestamp
kubectl logs job/secret-generator-<hash> -n global-secrets
```

**Force re-run:**
```bash
kubectl patch application global-secrets -n argocd --type=merge \
  -p '{"metadata":{"annotations":{"argocd.argoproj.io/refresh":"hard"}}}'
```

### Exporter Issues

**Check exporter logs:**
```bash
kubectl logs deployment/proxmox-exporter -n monitoring-system
kubectl logs deployment/opnsense-exporter -n monitoring-system
kubectl logs deployment/adguard-exporter -n monitoring-system
```

**Test Prometheus targets:**
```bash
kubectl port-forward svc/prometheus-operated -n monitoring-system 9090:9090
# Open http://localhost:9090/targets
```

### ExternalSecret Issues

**Check sync status:**
```bash
kubectl get externalsecrets -A
kubectl describe externalsecret <name> -n <namespace>
```

**Force refresh:**
```bash
kubectl annotate externalsecret <name> -n <namespace> \
  force-sync=$(date +%s) --overwrite
```
